%% ====================================================================
\section{Experimentation and Results}
\label{section:experiments}
%% ====================================================================

This section describes the tests we ran in order to test the
limitations of the Knox-ePouta connection presented in
Section~\ref{section:method}.
%
The set of VMs stretches across Sweden and Finland. The VMs are
connected to a dedicated network and communicate transparently with
each other.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Realistic Pipelines}
\label{section:experiments:pipelines}

The two workflows we tested, %
% CAW~\cite{caw} and WGS~\cite{wgs},
CAW and WGS, %
use Slurm~\cite{slurm} to schedule and manage the different
jobs across the available compute nodes.
%
Slurm can use partitions in order to group nodes into logical
(possibly overlapping) sets, where priorities and other constraints are
specified.
%
After installing all the necessary dependencies for these workflows on
the compute nodes and on the supernode, we distribute the compute
nodes in slurm partitions as in
Table~\ref{experiments:slurm:partitions}.
%
Moreover, when a task from a workflow only requires one node, we are
interested in finding out if the first node on the slurm partition
matters: we therefore added two more paritions, marked with * in
Table~\ref{experiments:slurm:partitions}, where the ePouta nodes are
effectively given a~higher priority.

\begin{table}[ht]%[!b]
\caption{Slurm Partitions}
\label{experiments:slurm:partitions}
\centering
\begin{tabular}{|r||c|c|l|}\hhline{*{3}{=}}
Partition       & \#nodes on Knox & \#nodes on ePouta \\\hhline{*{3}{=}}
knox            & 3               & 0                 \\
knox2-epouta1   & 2               & 1                 \\
knox1-epouta2   & 1               & 2                 \\
epouta          & 0               & 3                 \\\hline
epouta1-knox2 * & 1               & 2                 \\
epouta2-knox1 * & 2               & 1                 \\\hhline{*{3}{=}}
\multicolumn{3}{l}{\scriptsize * ePouta nodes have higher priority than Knox nodes}\\
\end{tabular}
\end{table}

%Both CAW~\cite{caw} and WGS~\cite{wgs} %
CAW and WGS %
run using Nextflow~\cite{nextflow}.
%
We report the elapsed time for each partition in
Table~\ref{experiments:CAW+WGS} when we experimented with a small
non-sensitive data sample.

\codeblock{caw-wgs.run.sh}

\begin{table}[ht]%[!b]
\caption{Results per partition for CAW (left) and WGS (right)}
\label{experiments:CAW+WGS}
\begin{minipage}{0.5\linewidth}
\centering
\begin{tabular}{|l|rr|}\hhline{*{3}{=}}
Partition     & \multicolumn{2}{r|}{Elapsed Time}\\\hhline{*{3}{=}}
knox          & & 23m 58s \\
knox2-epouta1 & & 23m 27s \\
knox1-epouta2 & & 22m 55s \\
epouta        & \leftpointingfinger & 15m 32s \\
epouta1-knox2 & & 17m 28s \\
epouta2-knox1 & & 15m 27s \\\hhline{*{3}{=}}
\end{tabular}
\end{minipage}%
%
\begin{minipage}{0.5\linewidth}
\centering
\begin{tabular}{|l|rr|}\hhline{*{3}{=}}
Partition     & \multicolumn{2}{r|}{Elapsed Time}\\\hhline{*{3}{=}}
knox          & & 8m 40s \\
knox2-epouta1 & & 9m 43s \\
knox1-epouta2 & & 9m 42s \\
epouta        & \leftpointingfinger & 10m 14s\\
epouta1-knox2 & \leftpointingfinger & 11m 11s \\
epouta2-knox1 & \leftpointingfinger & 11m 13s \\\hhline{*{3}{=}}
\end{tabular}
\end{minipage}
\end{table}

Since we know that the node in ePouta are technically superior to the
ones in Knox (\ie better hardware), we can observe that the
ePouta-Knox connection across borders does not influence much the
computations. Moreover, the first compute node on the slurm partition
appears to \emph{not} matter.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{NFS stress test}
\label{experiments:NFS}

We suspected the network file system (NFS) to be a bottleneck in the
previous workflows, since they require a lot of disk access (and
comparatively not so much computing power). So we stress-tested NFS by
writing files from the compute nodes onto an NFS-shared location --the
storage node-- using
\href{https://www.pdc.kth.se/~pek/sob}{SOB}~\cite{sob}.
%
We used the following tests:

\begin{tests}
%
\item\label{experiments:SOB:test:big}%
  Write a 10GB file in chunks of 8MB. Basic test of write I/O
  bandwidth.  For this kind of test it is important that the file is
  substantially larger than the main memory of the machine. If the
  file is 2GB and main memory is 1GB then up to 50\% of the file could
  be cached by the operating system and the reported write bandwidth
  would be much higher than what the disk+filesystem could actually
  provide.
  % 
  \\\textbf{Command:}\ \codeline{sob\ -rw\ -b\ 8m\ -s\ 10g}
  % 
\item\label{experiments:SOB:test:dir}%
  Writing 500 files of 1MB, spread out in 10 directories
  % 
  \\\textbf{Command:}\ \codeline{sob\ -w\ -b\ 64k\ -s\ 1m\ -n\ 500\ -o\ 50}
  % 
\item\label{experiments:SOB:test:random}%
  Write 50 files, of 128MB each (\ie a total of 6.4GB), with a block
  size of 64kB, then read 5000 times some files randomly picked. This
  is a good way to test random access and mask buffer cache effects
  (provided the sum size of all the files is much larger than main
  memory).
  % 
  \\\textbf{Command:}\ \codeline{sob\ -w\ -R\ 5000\ -n\ 50\ -s\ 128m\ -b\ 64k}
  % 
\item\label{experiments:SOB:test:cache}%
  Read and write 1 file of 1 GB (Checking whether the file is cached in memory).
  % 
  \\\textbf{Command:}\ \codeline{sob\ -rw\ -b\ 128k\ -s\ 1g}
  % 
\end{tests}

We wrote a script which, given a set of VMs, connects via \texttt{ssh}
to the VMs (in the specified order) and runs a SOB test on
each VM in parallel.
%
It is worth mentioning that we limited the effect of the NFS caching
mechanism, by unmounting and remounting the NFS share before each
test.
%
We report in Table~\ref{experiments:SOB:tests} the writing times for
each test and each VM of the chosen set.
%
The VM names are \codeline{knox$_i$} (resp.~\codeline{epouta$_i$}) if it was
booted on Knox (resp.\ ePouta), where $i\in\{1,2,3\}$.
%
The sets correspond roughly to the four first partitions from
Table~\ref{experiments:slurm:partitions}.
%
For test~\ref{experiments:SOB:test:big} and
\ref{experiments:SOB:test:cache}, the second line represents the
reading time.
%
The numbers in MB/s indicate the bandwidth (rounded to the closest
first decimal).

\begin{table}[ht]%[!b]
\caption{SOB tests running on two different sets}
\label{experiments:SOB:tests}
\resultpartition%
{{ePouta3}{ePouta2}{epouta1}}%
{{ 527 }{ 19.4   }{ 399 }{ 25.7   }{ 363 }{ 28.2   }}%
{{ 217 }{ 47.1   }{ 300 }{ 34.1   }{ 287 }{ 35.6   }}%
{{  79 }{ 6.3    }{  72 }{ 6.9    }{  75 }{ 6.7    }}%
{{ 267 }{ 24.0   }{ 259 }{ 24.8   }{ 267 }{ 24.0   }}%
{{  31 }{ 33.4   }{  38 }{ 26.9   }{  34 }{ 29.8   }}%
{{ 0.4 }{ 2583.8 }{ 0.3 }{ 3702.7 }{ 0.3 }{ 4053.6 }}

\resultpartition%
{{knox1}{knox2}{knox3}}%
{{ 212 }{ 48.4   }{ 487 }{ 21.0   }{ 492 }{ 20.8   }}%
{{ 166 }{ 61.8   }{ 219 }{ 46.7   }{ 208 }{ 49.3   }}%
{{  55 }{ 9.0    }{  60 }{ 8.4    }{  60 }{ 8.3    }}%
{{ 204 }{ 31.3   }{ 237 }{ 27.0   }{ 243 }{ 26.4   }}%
{{  26 }{ 33.7   }{  39 }{ 26.3   }{  37 }{ 27.8   }}%
{{ 0.3 }{ 3330.5 }{ 0.3 }{ 3379.5 }{ 0.3 }{ 3703.7 }}
\end{table}

For clarity, we only report the results for two sets/partitions. The
others are found in the appendix~\ref{appendix:SOB}. These tests
reveal that the use of NFS is not necessarily a bottleneck: if the
workflows do not write big files, the system should hold the load,
whether computations are performed in Sweden or in Finland, with data
in Sweden.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Testing the 1GB-link}
\label{section:experiments:link}

Finally, we test the network link between the VMs.
%
The first test consists in opening 10 connections between
\texttt{epouta1} and \texttt{knox1} and holding them during 60
seconds.  One machine acts as the server, and the other one connects
to it, as a sender as well as a receiver. We used iPerf~\cite{iperf}
with the following commands:

\codeblock{iperf-ke1.sh}

The results are surprising: We get near the physical link speed! After
summing the different connections, we reached 941~Mbits/sec as
sender and 937~Mbits/sec as receiver. To compare, running the
same test between \texttt{knox2} and \texttt{knox1}, we get
942~Mbits/sec as sender and 938~Mbits/sec as
receiver.

The last test is to make three connections, pairwise between one node
from Knox and one from ePouta, and run similar iPerf commands as
above.

\begin{center}
\begin{tabular}{|r||c||c|}\hhline{~||-||-}
\multicolumn{1}{c|}{Connection} & Sender         & Receiver\\\hhline{-::=::=}
epouta1 \vmconnect\ knox1 & 275 Mbits/sec  & 272 Mbits/sec\\
epouta2 \vmconnect\ knox2 & 281 Mbits/sec  & 278 Mbits/sec\\
epouta3 \vmconnect\ knox3 & 452 Mbits/sec  & 447 Mbits/sec\\\hhline{-::=::=}
\multicolumn{1}{r|}{Total} & 1008 Mbits/sec & 997 Mbits/sec\\\hhline{~::-::-}
\end{tabular}
\end{center}

We get similar speed between \texttt{knox1} and \texttt{knox2}, which
happened to be scheduled on different physical nodes in Knox.
%
In conclusion, the VMs' network is smoothly scaling at near
link-speed.
