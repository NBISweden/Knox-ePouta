%% ====================================================================
\section{Implementation using OpenStack}
\label{section:implementation}

In this section, we present how we implemented the method from
Section~\ref{section:method}, using two OpenStack environments.
%
The openstack version in place in both Knox and ePouta is
\href{http://docs.openstack.org/liberty/install-guide-ubuntu/}{Liberty}.
%
% This section is included in the paper for the readers that intend to
% reproduce the results using two OpenStack environments.
%
You can refer to the
\href{https://github.com/NBISweden/Knox-ePouta}{NBIS GitHub
  repository}~\cite{nbis-knox-epouta} for the source code, in order to
reproduce the setup and re-run the tests.
%
We start with the low-level component: we connect Knox
and ePouta via a 1Gb/s fiber-link.
%
We use an MPLS tunnel (at OSI layer 2) between
\href{https://www.sunet.se/}{SUNET} and
\href{https://www.csc.fi/funet-network-services}{FUNET}.
%
The dedicated network uses the \ip{10.101.0.0/16} CIDR for communication
between the VMs.
%
We split the latter IP address range in two disjoint parts: VMs on
ePouta use the lower part of the range, while VMs on Knox use the
higher part. The exception is the virtual router.
%
The DHCP server on Knox (resp. ePouta) provides network information
for the VMs on Knox (resp. ePouta).
%
We give the following IPs to the different components:
%
\begin{itemize}
\item the Virtual Router at \ip{10.101.0.1} (on Knox),
\item the Knox DHCP server is at \ip{10.101.128.0},
\item the ePouta DHCP server is at \ip{10.101.0.3},
\item the set of VMs on ePouta\\ranging from \ip{10.101.0.4} to \ip{10.101.127.255},% and
\item the set of VMs on Knox\\ranging from \ip{10.101.128.1} to \ip{10.101.128.254}.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Virtual Router and DHCP server, \emph{on Knox}}
\label{section:implementation:virtual:router}
\label{section:implementation:dhcp:server:knox}

OpenStack uses the network namespace capabilities of the Linux kernel,
in order to isolate routes, firewall rules and interfaces from the
root namespace.
%
This is where the virtual router and the dhcp servers live, each in
its own namespace.
%
Moreover, we use the Neutron Linux Bridge driver in OpenStack,
configured to encapsulate ethernet communications with VLAN.
%
In this setup, we choose the VLAN tag 1203.
%
The naming convention is such that network components (often) start
with \codeline{q}, for historical reasons: \emph{Neutron} used to be
called \emph{Quantum}. We use \codeline{\uuid}
to denote a \emph{universally unique identifier} (such as
\codeline{1a6abf7e-f927-4598-9a6c-c4311e685e52}), otherwise we use
\codeline{\textless{}\emph{description}\textgreater{}} whenever a
parameter must be substituted with \emph{description} by the reader.

\begin{figure}[ht]%!b
  \centering
  \input{img/controller}
  \caption{Connectivity on the controller}
  \label{figure:connectivity:controller}
\end{figure}


The following Neutron OpenStack commands must be run on Knox
\codeline{controller} to create the environment on Knox side.
%
Essentially,
\begin{dazenumerate}
\item we include the credentials to connect to Knox's OpenStack,
\item we create the virtual router,
\item we instantiate a physical network with VLAN tag 1203,
\item we specify the above IP range\\from \ip{10.101.128.0} to
  \ip{10.101.255.254},\\where the gateway is \ip{10.101.0.1}, and
\item we add an interface to the router to access that network.
\end{dazenumerate}

\codeblock{neutron-commands-knox.sh}

We show now the underlying components that the Neutron driver created
with the above commands.
%   , \ie network namespaces, bridges, and veth
% pairs dedicated to the \ip{10.101.0.0/16} network.
%
First of all, we show
%\codeline{ip netns} shows
the two newly created
namespaces %, \codeline{qrouter-\uuid} and \codeline{qdhcp-\uuid},
for the virtual router and the DHCP server. % respectively.

\codeblock{ip-netns.sh}

Both the virtual router and the DHCP server are connected to the root
namespace via 2 tap interfaces, added to a common bridge. OpenStack
also created a VLAN interface (with tag 1203) on that bridge for the
\ip{10.101.0.0/16} network.
%
The Neutron driver actually created veth pairs, where one end is moved
to a network namespace, and the other end (the \codeline{tap}
interface) is still in the root namespace. The end in the router
namespace is of the form \texttt{qr-\textless{}...\textgreater{}} and
the one in the DHCP namespace is of the form
\texttt{ns-\textless{}...\textgreater{}}. The other end of the veth
pairs, in the root namespace, is added to a Linux bridge. OpenStack
creates a Linux bridge per project.%
\footnote{%
  We think we found a problem with MAC addresses on the Linux bridge
  in Ubuntu: The tap interface connected to the virtual router is
  learned on the wrong port of the bridge. Moreover, the MAC address
  of the bridge itself is by construction the lowest one of all its
  interfaces (unless fixed at creation). Updating the Neutron driver
  for fixing the bridge's MAC address at creation was out-of-scope for
  that project. Instead, we opted for the following quickfix: We
  disabled the MAC learning algorithm of the bridge, and made it
  behave like a hub (and not a virtual switch).
  %
  The following command (as root) provides a solution (albeit
  non-optimal): 
  %\codeblock{bridge-ageing.sh}
  \codeline{brctl setageing brq\uuid\ 0}\\
  %
  \noindent%
  We don't recall that it was necessary on CentOS. 
  % 
}
%
% Finally, the driver makes sure that the outgoing interface of the
% physical host uses the VLAN tag 1203, and is also added to that same
% bridge, therefore providing connectivity and security to the router
% (over that VLAN).

\codeblock{bridge.sh}

% The DHCP namespace isolates a \texttt{dnsmasq} process, while the
% other namespace isolates routes and IPtables rules for the virtual router.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{VM connectivity on the compute nodes}
\label{section:implementation:compute:nodes}

On each compute node, a bridge is created (also per project), along
with an interface with VLAN tag 1203. A veth pair's end is added to
the bridge, while to other end is used by a VM, as its internal
interface. We illustrate it in
Figure~\ref{figure:connectivity:compute:nodes}.

\codeblock{bridge-cn.sh}

A kernel setting is used to force IPtables to filter traffic on the
bridge. This is the way OpenStack enforces security groups and in
particular ensures some address spoofing protection. These rules can
be slightly manipulated by updating the Neutron port settings.
%
It is relevant in our case, because this led to a slew of connection
issues, whether in between VMs from the same cloud or across
clouds. However, this part is only technical and is found in the
Appendix~\ref{appendix:secgroups}.

\begin{figure}[ht]%!b
  \centering
  \input{img/compute-node}
  \caption{Connectivity on the compute nodes. VLAN encapsulation
    ensures that VMs from different projects do not ``see'' each
    other, even when belonging to the same IP range.}
  \label{figure:connectivity:compute:nodes}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{External connectivity for the VMs}
\label{section:implementation:external:connectivity}

All VMs have a default route to the virtual router. Therefore,
external connectivity is adjusted in the virtual router's namespace
(on Knox).
%
OpenStack usually creates a veth pair, where one end is a
\emph{gateway} interface added to the virtual router, and the router
translates the source address (\emph{source NATing} or \emph{SNAT}),
using IPTables, for outgoing traffic over that interface. Traffic to
the \ip{10.101.0.0/16} network is routed through the
\codeline{qr-\uuid} interface, and all other traffic is routed through
the gateway interface.
%
The other end of the veth pair is still in the root namespace, and is
added to an \emph{external} bridge, which already forwards traffic to
the host's external interface. That way, all virtual routers, also
potentially connected to that bridge, have external connectivity.
%
However, in our case, we did not need to use this (classic) OpenStack
setup. We instead used a single veth pair%
%(denoted \codeline{gw~{\veth}~mm})%
, along with another \emph{local} network
\ip{10.5.0.0/24}. The \codeline{gw} end belongs to the virtual router
and has the IP \ip{10.5.0.2}, while \codeline{mm} has \ip{10.5.0.1}.
%
On the controller:

\codeblock{gw-mm.sh}

Inside the virtual router:

\codeblock{gw-mm-vr.sh}

The routes in the virtual router are so far:

\codeblock{routes-vr.sh}

We chose to not give a full external connectivity to the VMs. We do
not have a \kw{default} route in the routing table of the virtual
router. Instead, we only added a few extra routes as follows (We omit
the actual IP addresses).

\codeblock{routes-vr-extra.sh}

Finally, for external connectivity, it is necessary to \emph{source
  NAT} the traffic coming out of the virtual router.

\codeblock{snat.sh}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Settings on ePouta}
\label{section:implementation:epouta}

The network settings to boot VMs on ePouta are prepared similarly to
Section~\ref{section:implementation:dhcp:server:knox}.
%
It is however necessary to adjust the parameters, in order to make the
VMs in ePouta occupy the lowest part of the IP range
\ip{10.101.0.0/16}.
%
Notice then that we also adjust the DNS setting. In the namespace
related to the DHCP server lies a \kw{dnsmasq} process, which accepts
DNS queries and either answers them from a small, local, cache or
forwards them to a real, recursive, DNS server. Therefore, we make the
VMs DNS queries point to \ip{10.101.128.0}.

\codeblock{epouta.sh}

